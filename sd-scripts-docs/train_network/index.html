<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Lora-模型训练 - Models Train Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">Models Train Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Home</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">SD-Scripts-Docs</a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">数据准备</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../config/" class="dropdown-item">数据集配置</a>
</li>
            
<li>
    <a href="../masked_loss/" class="dropdown-item">蒙版数据</a>
</li>
            
<li>
    <a href="../wd14_tagger/" class="dropdown-item">打标</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">模型训练</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../fine_tune/" class="dropdown-item">微调训练</a>
</li>
            
<li>
    <a href="../train/" class="dropdown-item">模型训练</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active" aria-current="page">Lora-模型训练</a>
</li>
            
<li>
    <a href="../train_db/" class="dropdown-item">DreamBooth-模型训练</a>
</li>
            
<li>
    <a href="../train_ti/" class="dropdown-item">Textual-Inversion-模型训练</a>
</li>
            
<li>
    <a href="../train_lllite/" class="dropdown-item">ControlNet-LLLite-模型训练</a>
</li>
            
<li>
    <a href="../train_SDXL/" class="dropdown-item">SDXL-模型训练</a>
</li>
            
<li>
    <a href="../sdxl_train_network/" class="dropdown-item">SDXL-Lora-模型训练</a>
</li>
            
<li>
    <a href="../sd3_train_network/" class="dropdown-item">SD3-模型训练</a>
</li>
            
<li>
    <a href="../flux_train/" class="dropdown-item">Flux-模型训练</a>
</li>
    </ul>
  </li>
                                    
<li>
    <a href="../gen_img/" class="dropdown-item">图像生成</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../train/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../train_db/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/SD-Model-Tools/sd-moadel-doc_zh_cn/edit/master/docs/sd-scripts-docs/train_network.md" class="nav-link"><i class="fa-brands fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#lora" class="nav-link">LoRA的训练</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#lora_1" class="nav-link">可以训练的LoRA类型</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#_1" class="nav-link">关于训练后的模型的注意事项</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#_2" class="nav-link">训练步骤</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#_3" class="nav-link">数据准备</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_4" class="nav-link">执行训练</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#_5" class="nav-link">其他训练方法</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#lora-c3lier" class="nav-link">训练LoRA-C3Lier</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#dylora" class="nav-link">DyLoRA</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_6" class="nav-link">层次别学习率</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#dim-rank" class="nav-link">层次别dim (rank)</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#_8" class="nav-link">其他脚本</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#_9" class="nav-link">关于合并脚本</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#ranklora" class="nav-link">合并多个rank不同的LoRA模型</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_11" class="nav-link">使用本仓库中的图像生成脚本生成</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#diffuserspipeline" class="nav-link">在Diffusers的pipeline中生成</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#lora_3" class="nav-link">从两个模型差异创建LoRA模型</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_14" class="nav-link">图像大小调整脚本</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="1"><a href="#_17" class="nav-link">附加信息</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#cloneofsimo" class="nav-link">与cloneofsimo仓库的区别</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_18" class="nav-link">关于未来扩展</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="lora">LoRA的训练</h1>
<p>这是将<a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>（arxiv）和<a href="https://github.com/microsoft/LoRA">LoRA</a>（github）应用于Stable Diffusion的结果。</p>
<p>我大量参考了<a href="https://github.com/cloneofsimo/lora">cloneofsimo的仓库</a>，在此表示感谢。</p>
<p>通常的LoRA仅适用于Linear和内核大小为1x1的Conv2d，但也可以扩展到内核大小为3x3的Conv2d。</p>
<p>将Conv2d 3x3扩展到LoRA是由<a href="https://github.com/cloneofsimo/lora">cloneofsimo</a>首先发布的，<a href="https://github.com/KohakuBlueleaf/LoCon">KohakuBlueleaf</a>揭示了其有效性。在此对KohakuBlueleaf表示深深的感谢。</p>
<p>即使在8GB VRAM下，似乎也能勉强运行。</p>
<p>请同时参阅<a href="../train/">关于训练的共同文档</a>。</p>
<h1 id="lora_1">可以训练的LoRA类型</h1>
<p>支持以下两种类型。以下是在本仓库中的自定义名称。</p>
<ol>
<li>
<p><strong>LoRA-LierLa</strong>：（LoRA for <strong>Li__n __e__a __r</strong>  <strong>La</strong> yers，读作LierLa）</p>
<p>适用于Linear和内核大小为1x1的Conv2d的LoRA。</p>
</li>
<li>
<p><strong>LoRA-C3Lier</strong>：（LoRA for <strong>C__olutional layers with __3</strong> x3 Kernel and  <strong>Li__n __e__a __r</strong> layers，读作C3Lier）</p>
<p>在1的基础上，额外适用于内核大小为3x3的Conv2d的LoRA。</p>
</li>
</ol>
<p>与LoRA-LierLa相比，LoRA-C3Lier由于适用的层增加，可能期待更高的精度。</p>
<p>在训练时，也可以使用 <strong>DyLoRA</strong>（后述）。</p>
<h2 id="_1">关于训练后的模型的注意事项</h2>
<p>LoRA-LierLa可以被AUTOMATIC1111的Web UI的LoRA功能使用。</p>
<p>要使用LoRA-C3Lier在Web UI中生成图像，请使用这个<a href="https://github.com/kohya-ss/sd-webui-additional-networks">WebUI扩展</a>。</p>
<p>两者都可以使用本仓库中的脚本将训练好的LoRA模型与Stable Diffusion模型合并。</p>
<p>目前与cloneofsimo的仓库和d8ahazard的<a href="https://github.com/d8ahazard/sd_dreambooth_extension">Dreambooth Extension for Stable-Diffusion-WebUI</a>不兼容，因为进行了一些功能扩展（后述）。</p>
<h1 id="_2">训练步骤</h1>
<p>请先参考本仓库的README，进行环境准备。</p>
<h2 id="_3">数据准备</h2>
<p>请参考<a href="../train/">关于训练数据准备</a>。</p>
<h2 id="_4">执行训练</h2>
<p>使用<code>train_network.py</code>。</p>
<p>在<code>train_network.py</code>中，通过<code>--network_module</code>选项指定要训练的模块名称。对于LoRA，指定<code>network.lora</code>。</p>
<p>学习率似乎比通常的DreamBooth或fine tuning要高，建议使用<code>1e-4</code>~<code>1e-3</code>左右的值。</p>
<p>以下是一个命令行示例。</p>
<pre><code>accelerate launch --num_cpu_threads_per_process 1 train_network.py 
    --pretrained_model_name_or_path=&lt;.ckpt或.safetensors或Diffusers版模型的目录&gt; 
    --dataset_config=&lt;数据准备时创建的.toml文件&gt; 
    --output_dir=&lt;训练好的模型的输出文件夹&gt;  
    --output_name=&lt;训练好的模型输出时的文件名&gt; 
    --save_model_as=safetensors 
    --prior_loss_weight=1.0 
    --max_train_steps=400 
    --learning_rate=1e-4 
    --optimizer_type=&quot;AdamW8bit&quot; 
    --xformers 
    --mixed_precision=&quot;fp16&quot; 
    --cache_latents 
    --gradient_checkpointing
    --save_every_n_epochs=1 
    --network_module=networks.lora
</code></pre>
<p>这个命令行将训练LoRA-LierLa。</p>
<p>LoRA模型将被保存到<code>--output_dir</code>选项指定的文件夹中。其他选项和优化器等请参考<a href="../train/">训练的共同文档</a>的“常用选项”。</p>
<p>此外，还可以指定以下选项。</p>
<ul>
<li><code>--network_dim</code></li>
<li>指定LoRA的RANK（例如<code>--network_dim=4</code>）。省略时默认为4。数值越大表示能力越强，但训练所需的内存和时间也会增加。盲目增加数值也不一定好。</li>
<li><code>--network_alpha</code></li>
<li>指定用于防止下溢并稳定训练的<code>alpha</code>值。默认为1。`</li>
<li>当与<code>network_dim</code>的值相同指定时，行为与以前的版本相同。</li>
<li><code>--persistent_data_loader_workers</code></li>
<li>在Windows环境中指定时，epoch之间的等待时间将大大缩短。</li>
<li><code>--max_data_loader_n_workers</code></li>
<li>指定数据加载的进程数。进程数越多，数据加载越快，GPU利用率越高，但会消耗主内存。默认值为“<code>8</code>或<code>CPU并发线程数-1</code>中较小的一个”，因此如果主内存不充裕或GPU利用率达到90%左右，请根据这些数值，将其降低到<code>2</code>或<code>1</code>左右。</li>
<li><code>--network_weights</code></li>
<li>在训练前加载预训练的LoRA权重，并在此基础上继续训练。</li>
<li><code>--network_train_unet_only</code></li>
<li>只启用与U-Net相关的LoRA模块。在fine tuning式的训练中指定可能比较好。</li>
<li><code>--network_train_text_encoder_only</code></li>
<li>只启用与Text Encoder相关的LoRA模块。可能会期待Textual Inversion式的效果。</li>
<li><code>--unet_lr</code></li>
<li>在与U-Net相关的LoRA模块中使用与通常的学习率（通过<code>--learning_rate</code>选项指定）不同的学习率时指定。</li>
<li><code>--text_encoder_lr</code></li>
<li>在与Text Encoder相关的LoRA模块中使用与通常的学习率（通过<code>--learning_rate</code>选项指定）不同的学习率时指定。Text Encoder的学习率稍微低一些（5e-5等）可能比较好。</li>
<li><code>--network_args</code></li>
<li>可以指定多个参数，后述。</li>
<li><code>--alpha_mask</code></li>
<li>将图像的alpha值用作掩码。在训练透明图像时使用。<a href="https://github.com/kohya-ss/sd-scripts/pull/1223">PR #1223</a></li>
</ul>
<p>当<code>--network_train_unet_only</code>和<code>--network_train_text_encoder_only</code>都没有指定（默认）时，Text Encoder和U-Net的LoRA模块都会被启用。</p>
<h1 id="_5">其他训练方法</h1>
<h2 id="lora-c3lier">训练LoRA-C3Lier</h2>
<p>请按如下方式指定<code>--network_args</code>。用<code>conv_dim</code>指定Conv2d (3x3)的rank，用<code>conv_alpha</code>指定alpha。</p>
<pre><code>--network_args &quot;conv_dim=4&quot; &quot;conv_alpha=1&quot;
</code></pre>
<p>在以下示例中，省略<code>alpha</code>时默认为1。</p>
<pre><code>--network_args &quot;conv_dim=4&quot;
</code></pre>
<h2 id="dylora">DyLoRA</h2>
<p>DyLoRA是<a href="https://arxiv.org/abs/2210.07558">这篇论文</a>中提出的。官方实现在<a href="https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA">这里</a>。</p>
<p>根据论文，LoRA的rank不一定是越高越好，需要根据目标模型、数据集、任务等寻找合适的rank。使用DyLoRA，可以同时训练多个rank的LoRA，从而省去分别训练和寻找最佳rank的麻烦。</p>
<p>本仓库的实现基于官方实现，并进行了一些扩展（因此可能存在bug等）。</p>
<h3 id="dylora_1">本仓库中DyLoRA的特点</h3>
<p>训练后的DyLoRA模型文件与LoRA兼容。此外，可以从模型文件中提取出多个dim的LoRA。</p>
<p>可以训练DyLoRA-LierLa和DyLoRA-C3Lier。</p>
<h3 id="dylora_2">使用DyLoRA训练</h3>
<p>指定<code>--network_module=networks.dylora</code>，像这样使用DyLoRA对应的<code>network.dylora</code>。</p>
<p>此外，通过<code>--network_args</code>指定<code>unit</code>，例如<code>--network_args "unit=4"</code>。<code>unit</code>是分割rank的单位。例如，指定<code>--network_dim=16 --network_args "unit=4"</code>。<code>unit</code>应该是<code>network_dim</code>的因数（<code>network_dim</code>是<code>unit</code>的倍数）。</p>
<p>如果不指定<code>unit</code>，则被视为<code>unit=1</code>。</p>
<p>示例如下。</p>
<pre><code>--network_module=networks.dylora --network_dim=16 --network_args &quot;unit=4&quot;

--network_module=networks.dylora --network_dim=32 --network_alpha=16 --network_args &quot;unit=4&quot;
</code></pre>
<p>对于DyLoRA-C3Lier，通过<code>--network_args</code>指定<code>conv_dim</code>，例如<code>"conv_dim=4"</code>。与通常的LoRA不同，<code>conv_dim</code>需要与<code>network_dim</code>相同。示例如下。</p>
<pre><code>--network_module=networks.dylora --network_dim=16 --network_args &quot;conv_dim=16&quot; &quot;unit=4&quot;

--network_module=networks.dylora --network_dim=32 --network_alpha=16 --network_args &quot;conv_dim=32&quot; &quot;conv_alpha=16&quot; &quot;unit=8&quot;
</code></pre>
<p>例如，当dim=16，unit=4时，可以训练和提取4、8、12、16四个rank的LoRA。通过比较不同rank的LoRA生成的图像，可以选择最佳的rank。</p>
<p>其他选项与通常的LoRA相同。</p>
<p>※ <code>unit</code>是本仓库的独有扩展。在DyLoRA中，与相同dim的通常LoRA相比，训练时间可能会更长，因此采用了更大的分割单位。</p>
<h3 id="dyloralora">从DyLoRA模型中提取LoRA模型</h3>
<p>使用<code>networks</code>文件夹中的<code>extract_lora_from_dylora.py</code>。按照指定的<code>unit</code>单位，从DyLoRA模型中提取LoRA模型。</p>
<p>命令行示例：</p>
<pre><code class="language-powershell">python networks\extract_lora_from_dylora.py --model &quot;foldername/dylora-model.safetensors&quot; --save_to &quot;foldername/dylora-model-split.safetensors&quot; --unit 4
</code></pre>
<p><code>--model</code>指定DyLoRA模型文件，<code>--save_to</code>指定提取的LoRA模型的保存文件名（将在文件名后附加rank数值），<code>--unit</code>指定DyLoRA训练时的<code>unit</code>。</p>
<h2 id="_6">层次别学习率</h2>
<p>详情请参阅<a href="https://github.com/kohya-ss/sd-scripts/pull/355">PR #355</a>。</p>
<p>可以为全部25个full模型的block指定权重。第一个block对应的LoRA不存在，但为了与层次别LoRA应用等保持兼容性，指定为25个。此外，即使不扩展到conv2d3x3，一部分block的LoRA也不存在，但为了统一描述，始终指定25个值。</p>
<p>在SDXL中，请指定down/up 9个，middle 3个数值。</p>
<p>通过<code>--network_args</code>指定以下参数。</p>
<ul>
<li><code>down_lr_weight</code>：指定U-Net的down blocks的学习率权重。可以指定：</li>
<li>每个block的权重：例如<code>"down_lr_weight=0,0,0,0,0,0,1,1,1,1,1,1"</code>，指定12个数值（SDXL中为9个）。</li>
<li>从预设指定：例如<code>"down_lr_weight=sine"</code>，使用sine曲线指定权重。可以指定sine、cosine、linear、reverse_linear、zeros。此外，通过添加<code>+数值</code>（例如<code>"down_lr_weight=cosine+.25"</code>），可以加上指定的数值（范围将是0.25~1.25）。</li>
<li><code>mid_lr_weight</code>：指定U-Net的mid block的学习率权重。例如<code>"mid_lr_weight=0.5"</code>，指定一个数值（SDXL中为3个）。</li>
<li><code>up_lr_weight</code>：指定U-Net的up blocks的学习率权重。与down_lr_weight相同。</li>
<li>省略的部分将被视为1.0。如果权重为0，则不会创建该block的LoRA模块。</li>
<li><code>block_lr_zero_threshold</code>：如果权重小于或等于此值，则不会创建LoRA模块。默认为0。</li>
</ul>
<h3 id="_7">层次别学习率命令行示例：</h3>
<pre><code class="language-powershell">--network_args &quot;down_lr_weight=0.5,0.5,0.5,0.5,1.0,1.0,1.0,1.0,1.5,1.5,1.5,1.5&quot; &quot;mid_lr_weight=2.0&quot; &quot;up_lr_weight=1.5,1.5,1.5,1.5,1.0,1.0,1.0,1.0,0.5,0.5,0.5,0.5&quot;

--network_args &quot;block_lr_zero_threshold=0.1&quot; &quot;down_lr_weight=sine+.5&quot; &quot;mid_lr_weight=1.5&quot; &quot;up_lr_weight=cosine+.5&quot;
</code></pre>
<h3 id="toml">层次别学习率toml文件示例：</h3>
<pre><code class="language-toml">network_args = [ &quot;down_lr_weight=0.5,0.5,0.5,0.5,1.0,1.0,1.0,1.0,1.5,1.5,1.5,1.5&quot;, &quot;mid_lr_weight=2.0&quot;, &quot;up_lr_weight=1.5,1.5,1.5,1.5,1.0,1.0,1.0,1.0,0.5,0.5,0.5,0.5&quot;,]

network_args = [ &quot;block_lr_zero_threshold=0.1&quot;, &quot;down_lr_weight=sine+.5&quot;, &quot;mid_lr_weight=1.5&quot;, &quot;up_lr_weight=cosine+.5&quot;, ]
</code></pre>
<h2 id="dim-rank">层次别dim (rank)</h2>
<p>可以为全部25个full模型的block指定dim (rank)。与层次别学习率类似，一部分block的LoRA可能不存在，但始终指定25个值。</p>
<p>在SDXL中，请指定23个数值。由于与<code>sdxl_train.py</code>的<a href="../train_SDXL/">层次别学习率</a>兼容，因此一部分block的LoRA不存在。对应关系为<code>0: time/label embed, 1-9: input blocks 0-8, 10-12: mid blocks 0-2, 13-21: output blocks 0-8, 22: out</code>。</p>
<p>通过<code>--network_args</code>指定以下参数。</p>
<ul>
<li><code>block_dims</code>：指定每个block的dim (rank)。例如<code>"block_dims=2,2,2,2,4,4,4,4,6,6,6,6,8,6,6,6,6,4,4,4,4,2,2,2,2"</code>，指定25个数值。</li>
<li><code>block_alphas</code>：指定每个block的alpha。与block_dims类似，指定25个数值。省略时使用<code>network_alpha</code>的值。</li>
<li><code>conv_block_dims</code>：将LoRA扩展到Conv2d 3x3时，指定每个block的dim (rank)。</li>
<li><code>conv_block_alphas</code>：将LoRA扩展到Conv2d 3x3时，指定每个block的alpha。省略时使用<code>conv_alpha</code>的值。</li>
</ul>
<h3 id="dim-rank_1">层次别dim (rank)命令行示例：</h3>
<pre><code class="language-powershell">--network_args &quot;block_dims=2,4,4,4,8,8,8,8,12,12,12,12,16,12,12,12,12,8,8,8,8,4,4,4,2&quot;

--network_args &quot;block_dims=2,4,4,4,8,8,8,8,12,12,12,12,16,12,12,12,12,8,8,8,8,4,4,4,2&quot; &quot;conv_block_dims=2,2,2,2,4,4,4,4,6,6,6,6,8,6,6,6,6,4,4,4,4,2,2,2,2&quot;

--network_args &quot;block_dims=2,4,4,4,8,8,8,8,12,12,12,12,16,12,12,12,12,8,8,8,8,4,4,4,2&quot; &quot;block_alphas=2,2,2,2,4,4,4,4,6,6,6,6,8,6,6,6,6,4,4,4,4,2,2,2,2&quot;
</code></pre>
<h3 id="dim-ranktoml">层次别dim (rank)toml文件示例：</h3>
<pre><code class="language-toml">network_args = [ &quot;block_dims=2,4,4,4,8,8,8,8,12,12,12,12,16,12,12,12,12,8,8,8,8,4,4,4,2&quot;,]

network_args = [ &quot;block_dims=2,4,4,4,8,8,8,8,12,12,12,12,16,12,12,12,12,8,8,8,8,4,4,4,2&quot;, &quot;block_alphas=2,2,2,2,4,4,4,4,6,6,6,6,8,6,6,6,6,4,4,4,4,2,2,2,2&quot;,]
</code></pre>
<h1 id="_8">其他脚本</h1>
<p>与LoRA相关的脚本群，如合并等。</p>
<h2 id="_9">关于合并脚本</h2>
<p>使用merge_lora.py可以将LoRA的训练结果与Stable Diffusion模型合并，或将多个LoRA模型合并。</p>
<p>对于SDXL，有sdxl_merge_lora.py可用。选项等相同，因此请将以下merge_lora.py替换为sdxl_merge_lora.py。</p>
<h3 id="lorastable-diffusion">将LoRA模型与Stable Diffusion模型合并</h3>
<p>合并后的模型可以像普通的Stable Diffusion的ckpt一样处理。例如，以下是命令行示例。</p>
<pre><code>python networks\merge_lora.py --sd_model ..\model\model.ckpt 
    --save_to ..\lora_train1\model-char1-merged.safetensors 
    --models ..\lora_train1\last.safetensors --ratios 0.8
</code></pre>
<p>使用Stable Diffusion v2.x模型进行训练并合并时，请指定--v2选项。</p>
<p>通过--sd_model选项指定要合并的Stable Diffusion模型文件（目前仅支持.ckpt或.safetensors，不支持Diffusers）。</p>
<p>通过--save_to选项指定合并后的模型的保存路径（根据扩展名自动判断是.ckpt还是.safetensors）。</p>
<p>通过--models指定训练好的LoRA模型文件。可以指定多个，按顺序合并。</p>
<p>通过--ratios指定每个模型的适用率（即模型权重对原模型的反映程度），取值为0~1.0。例如，如果过拟合，可以尝试降低适用率来改善。模型的数量应与指定的模型数量相同。</p>
<p>多个模型时的示例：</p>
<pre><code>python networks\merge_lora.py --sd_model ..\model\model.ckpt 
    --save_to ..\lora_train1\model-char1-merged.safetensors 
    --models ..\lora_train1\last.safetensors ..\lora_train2\last.safetensors --ratios 0.8 0.5
</code></pre>
<h3 id="lora_2">合并多个LoRA模型</h3>
<p>指定--concat选项，可以简单地将多个LoRA合并成一个新的LoRA模型。文件大小（以及dim/rank）将是指定LoRA的总大小（如果要在合并时改变dim (rank)，请使用<code>svd_merge_lora.py</code>）。</p>
<p>例如，以下是命令行示例。</p>
<pre><code>python networks\merge_lora.py --save_precision bf16 
    --save_to ..\lora_train1\model-char1-style1-merged.safetensors 
    --models ..\lora_train1\last.safetensors ..\lora_train2\last.safetensors 
    --ratios 1.0 -1.0 --concat --shuffle
</code></pre>
<p>指定--concat选项。</p>
<p>此外，添加--shuffle选项，权重将被打乱。如果不打乱，合并后的LoRA可以被分解回原来的LoRA，因此在拷贝学习等情况下，可能会导致原始训练数据被暴露。请注意。</p>
<p>通过--save_to选项指定合并后的LoRA模型的保存路径（根据扩展名自动判断是.ckpt还是.safetensors）。</p>
<p>通过--models指定训练好的LoRA模型文件。可以指定三个以上。</p>
<p>通过--ratios指定每个模型的比率（即模型权重对原模型的反映程度），取值为0~1.0。将两个模型一对一合并时，比率应为“0.5 0.5”。如果使用“1.0 1.0”，则总权重将过大，结果可能不理想。</p>
<p>使用v1训练的LoRA和v2训练的LoRA、或者rank（维度数）不同的LoRA不能合并。U-Net alone的LoRA和U-Net+Text Encoder的LoRA理论上可以合并，但结果未知。</p>
<h3 id="_10">其他选项</h3>
<ul>
<li><code>precision</code></li>
<li>可以指定合并计算时的精度，有float、fp16、bf16可选。省略时为了确保精度默认为float。如果要减少内存使用量，可以指定fp16/bf16。</li>
<li><code>save_precision</code></li>
<li>可以指定模型保存时的精度，有float、fp16、bf16可选。省略时与precision相同。</li>
</ul>
<p>还有其他一些选项，请使用--help查看。</p>
<h2 id="ranklora">合并多个rank不同的LoRA模型</h2>
<p>使用<code>svd_merge_lora.py</code>将多个LoRA近似为一个LoRA（无法完全再现）。例如，以下是命令行示例。</p>
<pre><code>python networks\svd_merge_lora.py 
    --save_to ..\lora_train1\model-char1-style1-merged.safetensors 
    --models ..\lora_train1\last.safetensors ..\lora_train2\last.safetensors 
    --ratios 0.6 0.4 --new_rank 32 --device cuda
</code></pre>
<p><code>merge_lora.py</code>的主要选项相同。以下选项是新增的。</p>
<ul>
<li><code>--new_rank</code></li>
<li>指定创建的LoRA的rank。</li>
<li><code>--new_conv_rank</code></li>
<li>指定创建的 Conv2d 3x3 LoRA 的 rank。省略时与 <code>new_rank</code> 相同。</li>
<li><code>--device</code></li>
<li>指定 <code>--device cuda</code> 以在GPU上进行计算。处理速度更快。</li>
</ul>
<h2 id="_11">使用本仓库中的图像生成脚本生成</h2>
<p>在gen_img_diffusers.py中添加<code>--network_module</code>和<code>--network_weights</code>选项。含义与训练时相同。</p>
<p>通过<code>--network_mul</code>选项指定0~1.0的数值，可以改变LoRA的应用率。</p>
<h2 id="diffuserspipeline">在Diffusers的pipeline中生成</h2>
<p>请参考以下示例。只需要networks/lora.py文件。Diffusers的版本可能需要是0.10.2。</p>
<pre><code class="language-python">import torch
from diffusers import StableDiffusionPipeline
from networks.lora import LoRAModule, create_network_from_weights
from safetensors.torch import load_file

# 如果ckpt是基于CompVis的，请事先使用tools/convert_diffusers20_original_sd.py转换为Diffusers。有关详细信息，请参阅--help。

model_id_or_dir = r&quot;model_id_on_hugging_face_or_dir&quot;
device = &quot;cuda&quot;

# 创建管道
print(f&quot;从{model_id_or_dir}创建管道...&quot;)
pipe = StableDiffusionPipeline.from_pretrained(model_id_or_dir, revision=&quot;fp16&quot;, torch_dtype=torch.float16)
pipe = pipe.to(device)
vae = pipe.vae
text_encoder = pipe.text_encoder
unet = pipe.unet

# 加载LoRA网络
print(f&quot;加载LoRA网络...&quot;)

lora_path1 = r&quot;lora1.safetensors&quot;
sd = load_file(lora_path1)   # 如果文件是.ckpt，请使用torch.load。
network1, sd = create_network_from_weights(0.5, None, vae, text_encoder,unet, sd)
network1.apply_to(text_encoder, unet)
network1.load_state_dict(sd)
network1.to(device, dtype=torch.float16)

# # 您也可以合并权重，而不是apply_to+load_state_dict。network.set_multiplier不起作用
# network.merge_to(text_encoder, unet, sd)

lora_path2 = r&quot;lora2.safetensors&quot;
sd = load_file(lora_path2) 
network2, sd = create_network_from_weights(0.7, None, vae, text_encoder,unet, sd)
network2.apply_to(text_encoder, unet)
network2.load_state_dict(sd)
network2.to(device, dtype=torch.float16)

lora_path3 = r&quot;lora3.safetensors&quot;
sd = load_file(lora_path3)
network3, sd = create_network_from_weights(0.5, None, vae, text_encoder,unet, sd)
network3.apply_to(text_encoder, unet)
network3.load_state_dict(sd)
network3.to(device, dtype=torch.float16)

# 提示
prompt = &quot;杰作，最佳质量，1个女孩，穿着白衬衫，看着观众&quot;
negative_prompt = &quot;糟糕的质量，最差的质量，糟糕的解剖结构，糟糕的手&quot;

# 执行管道
print(&quot;生成图像...&quot;)
with torch.autocast(&quot;cuda&quot;):
    image = pipe(prompt, guidance_scale=7.5, negative_prompt=negative_prompt).images[0]

# 如果未合并，您可以使用set_multiplier
# network1.set_multiplier(0.8)
# 并再次生成图像...

# 保存图像
image.save(r&quot;由Diffusers生成的图像.png&quot;)
</code></pre>
<h2 id="lora_3">从两个模型差异创建LoRA模型</h2>
<p>这是参考<a href="https://github.com/cloneofsimo/lora/discussions/56">这个讨论</a>实现的。公式直接使用（似乎使用奇异值分解进行近似）。</p>
<p>将两个模型（例如fine tuning前的模型和fine tuning后的模型）的差异近似为LoRA。</p>
<h3 id="_12">脚本执行方法</h3>
<p>请按如下方式指定。</p>
<pre><code>python networks\extract_lora_from_models.py --model_org base-model.ckpt
    --model_tuned fine-tuned-model.ckpt 
    --save_to lora-weights.safetensors --dim 4
</code></pre>
<p>通过<code>--model_org</code>选项指定原始的Stable Diffusion模型。应用创建的LoRA模型时，将使用此模型。.ckpt或.safetensors可以指定。</p>
<p>通过<code>--model_tuned</code>选项指定要提取差异的目标Stable Diffusion模型。例如，fine tuning或DreamBooth后的模型。.ckpt或.safetensors可以指定。</p>
<p>通过<code>--save_to</code>指定LoRA模型的保存路径。<code>--dim</code>指定LoRA的维度数。</p>
<p>生成的LoRA模型可以像训练好的LoRA模型一样使用。</p>
<p>如果两个模型的Text Encoder相同，则LoRA将仅为U-Net的LoRA。</p>
<h3 id="_13">其他选项</h3>
<ul>
<li><code>--v2</code></li>
<li>使用v2.x的Stable Diffusion模型时指定。</li>
<li><code>--device</code></li>
<li>指定<code>--device cuda</code>以在GPU上进行计算。处理速度更快（CPU下也不太慢，大约是2~数倍）。</li>
<li><code>--save_precision</code></li>
<li>指定LoRA的保存格式，有"float"、"fp16"、"bf16"可选。省略时默认为float。</li>
<li><code>--conv_dim</code></li>
<li>指定时将LoRA的应用范围扩展到Conv2d 3x3。指定Conv2d 3x3的rank。</li>
</ul>
<h2 id="_14">图像大小调整脚本</h2>
<p>（稍后将整理文档，目前先在这里写说明。）</p>
<p>作为Aspect Ratio Bucketing的功能扩展，可以将较小的图像保持原样作为训练数据。此外，收到将原始训练图像缩小后的图像添加到训练数据的报告和预处理脚本，因此进行了整理和添加。感谢bmaltais。</p>
<h3 id="_15">脚本执行方法</h3>
<p>请按如下方式指定。原始图像和调整大小后的图像将被保存到转换后的文件夹中。调整大小后的图像的文件名将附加调整大小后的分辨率（例如<code>+512x512</code>）。如果调整大小后的分辨率小于原始图像，则不会被放大。</p>
<pre><code>python tools\resize_images_to_resolution.py --max_resolution 512x512,384x384,256x256 --save_as_png 
    原始图像文件夹 转换后的文件夹
</code></pre>
<p>原始图像文件夹中的图像文件将被调整大小，以使其面积与指定的分辨率（可指定多个）相同，并保存到转换后的文件夹中。非图像文件将保持不变。</p>
<p>通过<code>--max_resolution</code>选项指定调整大小后的大小，如示例所示。面积将被调整为该大小。指定多个时，将分别调整大小为每个分辨率。例如，<code>512x512,384x384,256x256</code>将在转换后的文件夹中生成原始大小和调整大小后的大小×3，共4张图像。</p>
<p>通过<code>--save_as_png</code>选项指定以png格式保存。省略时将以jpeg格式（quality=100）保存。</p>
<p>通过<code>--copy_associated_files</code>选项指定时，将与图像文件名（除扩展名外）相同的其他文件（例如caption）以调整大小后的图像的文件名复制。</p>
<h3 id="_16">其他选项</h3>
<ul>
<li><code>divisible_by</code></li>
<li>调整大小后的图像大小（高度和宽度）将通过裁剪图像中心，使其可以被该值整除。</li>
<li><code>interpolation</code></li>
<li>指定缩小时的插值方法。可选<code>area</code>、<code>cubic</code>、<code>lanczos4</code>，默认为<code>area</code>。</li>
</ul>
<h1 id="_17">附加信息</h1>
<h2 id="cloneofsimo">与cloneofsimo仓库的区别</h2>
<p>截至2022/12/25，本仓库将LoRA的应用范围扩展到Text Encoder的MLP、U-Net的FFN和Transformer的in/out projection，从而增强了表示能力。但代价是内存使用量增加，8GB VRAM勉强够用。</p>
<p>此外，模块替换机制完全不同。</p>
<h2 id="_18">关于未来扩展</h2>
<p>不仅限于LoRA，还可以支持其他扩展。未来将考虑添加这些扩展。</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
